%! TEX root = thesis.tex
\chapter{Introduction}\label{sec:intro}

Knowledge discovery results obtained from a dataset (e.g., patterns,
clusters, anomalies) should be \emph{statistically validated} in order to ensure
that they are not just due to the randomness in the data-generating
process~\citep{GionisMMT07,HamalainenW19,PellegrinaRV19b}: the goal of the
analysis is to \emph{gain knowledge of this process through the observed
dataset}, rather than knowledge of the dataset itself.

One such validation subjects the results to \emph{statistical hypothesis
tests}~\citep[Ch.\ 10]{Wasserman05} (see \cref{sec:prelims:significant}):
results that pass the tests are marked as \emph{(statistically) significant}, as
there is evidence that they give information on the data generating process.

The significance of the results is assessed against a \emph{null model} (see
\cref{sec:prelims:significant}), which is a collection of \emph{possible}
datasets that the unknown process \emph{may} generate, and a probability
distribution over this collection. The null model captures, in some sense, what
is assumed or already known about the data generating process, and the results
are assessed against it to understand in what way they cannot be explained by
the existing knowledge or assumptions.

The choice of the null model made by the user must be \emph{deliberate and informed},
as the meaning of ``statistically significant'' depends on the null model. For
example, the results deemed significant under one null model cannot in general
be compared to those deemed significant under a different null model.
Nevertheless, as the aphorism says: ``all models are wrong, but some are
useful'' (George E.\,P.\ Box), and some null models may be more appropriate for
testing the significance of the results than others, because they more closely
represent the settings of the task. It is therefore important that users have
access to a variety of null models and are informed of the differences between
them, in order to be able to select the more appropriate one for their needs. In
this work, we introduce a null model for testing the significance of results
obtained from \emph{binary transactional datasets}, such as (but not limited to)
collections of frequent itemsets~\citep{AgrawalS94}. A null model is, in some
sense, independent from the task whose results one wants to validate, as a null
model is a set of datasets, not results, but is used to evaluate results. Our
model is more appropriate for evaluating the results of common tasks on
transactional datasets than existing ones~\citep{GionisMMT07}, as we explain in
the \emph{Contributions} paragraph and in \cref{sec:gionis}.

Many statistical hypothesis tests are based on
\emph{resampling}~\citep{WestfallY93}: they analyze multiple datasets drawn
from the null model distribution, in order to approximate the distribution of
the test statistic and compare its value in the observed data against such
distribution. Thus, it is necessary to develop computationally-efficient
procedures for sampling such datasets from the null model distribution.

\section{Contributions}

We study the problem of evaluating the significance of results extracted from
unlabeled transactional datasets using resampling-based statistical hypothesis
tests.

\begin{itemize}
  \item We introduce a novel null model that preserves transaction lengths and
    item supports, but does not suffer from  the \emph{distortion} introduced by
    equating the set of datasets and the set of binary matrices (see
    \cref{sec:related,sec:gionis}), and is therefore more appropriate for
    evaluating the results of knowledge discovery tasks on transactional
    datasets. The user can specify \emph{any} distribution over the set of
    datasets.
  \item We present \algo\ (for \emph{Di}stortion-\emph{F}ree
    \emph{S}wap \emph{R}andomization), a set of algorithms for sampling datasets
    from the null model we introduce. These algorithms are based on
    \emph{swap-randomization} (see \cref{sec:diffusr}) and take a
    Markov-Chain Monte-Carlo (MCMC) approach on appropriately-defined Markov
    chains (see \cref{sec:prelims:markov}) over either the set of binary
    matrices (for our ``na\"{\i}ve'' algorithm), or the set of datasets (for our
    ``refined'' algorithm), whose stationary distribution is the user-specified
    sampling distribution.
  \item Our algorithms can be seen as the core of procedures to assess the
    statistical validity of data mining results using a resampling
    approach~\citep{WestfallY93} (see \cref{sec:prelims:significant}). To show
    the usefulness of our algorithms, we focus on the task of evaluating the
    significance of the number of frequent itemsets in a dataset, by
    computing an empirical $p$-value of this statistic.
  \item The results of our experimental evaluation show that the distortion
      issue is very relevant and affects the outcome of the testing of data mining
      results. \algo\ does not suffer from it, while being fast, (empirically)
      rapidly-mixing, and scalable as the dataset grows.
\end{itemize}

\section{Related Work}\label{sec:related}

Evaluating the statistical significance of results obtained by mining
transactional datasets has been recognized as important soon after the first
efficient mining algorithms were introduced. For example, \citet{BrinMS97}
use the chi-square test to assess the significance of association rules, while
\citet{MegiddoS98} use statistical significance testing to measure the number of
false discoveries.

Many measures of interestingness, both for individual patterns and for sets of
patterns~\citep{KontonasiosSDB12,TanKS04,VreekenT14}, were introduced with the
goal of mitigating the pattern explosion observed when using frequency as a
proxy of interestingness, but they do not offer the strict statistical
guarantees that allow precise statements about the interestingness of a pattern
w.r.t.\ the data generation process.

The framework of statistical hypothesis testing~\citep[Ch.\ 10]{Wasserman05}
offers such guarantees, and is now the de facto standard taken by works in
statistically-significant pattern mining~\citep{HamalainenW19,PellegrinaRV19b}.
The area of statistical pattern mining can be split into two classes, depending
on whether each transaction in the dataset is associated to a (binary or real)
label. Our method \algo\ works on \emph{unlabeled} datasets thus it can be used
for mining significant patterns in this setting, but many works studied the
labeled
case~\citep{LlinaresLopezSPB15,MinatoUTTS14,PellegrinaV20,PellegrinaRV19a,TeradaOHTS13,WuHGLZY16,KomiyamaIANM17}.
To keep the presentation focused, we only discuss contributions for unlabeled
datasets. The tutorial by \citet{PellegrinaRV19b} offers an in-depth
presentation of the labeled case.

Mining significant patterns requires \emph{testing multiple hypotheses}, one per
pattern. Due to the randomness of the data generation process, and the fact that
one only has access to the observed dataset, it is necessary to accept the
possibility of wrongly marking a pattern as significant even if it is not, i.e.,
of making a \emph{false discovery} involving that pattern, or, equivalently,
that the pattern is a \emph{false positive}. Algorithms for significant pattern
mining must therefore give \emph{probabilistic guarantees} on the presence of
false positives in their output. The most common of such guarantees is a bound
on the \emph{Family-Wise Error Rate (FWER)}, i.e., on the probability that the
output collection contains \emph{any} false positives. In pattern mining,
\citet{Webb06,Webb07}, \citet{Hamalainen10}, \citet{LowKRKP13}, and many others
(see the tutorial by \citet{HamalainenW19}) present methods to control the FWER
using the \emph{Bonferroni correction}~\citep{Bonferroni37}. This approach, even
when refined~\citep{Holm79,Webb08}, does not take into account dependencies and
correlations among hypotheses, i.e., the \emph{structure} of the hypothesis
class, which is very rich for the classes associated to patterns. Additionally,
these classes contain many patterns that would never be marked as significant
(e.g., because they do not appear in the observed dataset), thus using the
Bonferroni correction, while controlling the FWER to the desired level, results
in low statistical power, i.e., in not marking many true significant patterns as
such. One alternative is to control the False Discovery Rate (FDR), i.e., the
expected number of false positives, as proposed by \citet{KirschMPPUV12}. The
FDR is a less stringent guarantee than the FWER, thus is only appropriate in
some settings, while controlling the FWER is required in many cases. Resampling
methods~\citep{WestfallY93} take into account the structure of the hypothesis
class and offer high statistical power, while still bounding the FWER\@. \algo\
is designed to be used in such methods.

The resampling method by \citet{GionisMMT07}, while it can be used as the  core
component of a significant pattern mining algorithm, it is, like our work, more
generic and useful for assessing the validity of data mining results. They
do not discuss controlling the FWER, a void filled by \citet{Hanhijarvi11}.
\citet{GionisMMT07} look at transactional datasets as binary matrices, and
consider the null model composed by \emph{all} such matrices with the same row
and column sums (a.k.a., margins) as the observed dataset. We discuss
\citet{GionisMMT07}'s method in detail in \cref{sec:gionis}. The issue with this
approach that we identify and solve is that the space of matrices with
constrained margins does not actually have a one-to-one correspondence with the
space of transactional datasets with the same transaction lengths and item
frequencies, thus creating a \emph{distortion} of the latter space, and
effectively leading to performing statistical tests w.r.t.\ a \emph{different}
null model. The statistical significance of the observed results depends on the
null model, thus the distortion may make results that are not actually
significant appear as such, and vice versa. Our algorithms are
\emph{distortion-free} and sample w.r.t.\ the intended null model.

The randomization approach by \citet{GionisMMT07} has been extended to other
types of data (database tables~\citep{OjalaGGM10}, real-valued and mixed-value
matrices~\citep{OjalaVKHM08,Ojala10},
graphs~\citep{HanhijarviGP09,GunnemannDJE12}, genomic
data~\citep{FerkingstadHS15}), to different kinds of patterns
(sequences~\citep{TononV19}, subgroups~\citep{DuivesteijnK11}), and to the
iterative setting~\citep{HanhijarviOVPTM09}.  The  distortion issue affecting
\citet{GionisMMT07}'s approach is also present in many of these methods, and our
proposed solution can be adapted to these cases. We discuss some of them in
\cref{sec:extensions}, though an in-depth treatment is an interesting direction
for future work.
